"""\nCyberbullying Detection System with Encrypted Reports\nMain detection module for identifying harmful content\n"""\n\nimport re\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nimport pickle\nimport os\nfrom datetime import datetime\n\nclass CyberbullyingDetector:\n    """Main class for detecting cyberbullying content"""\n    \n    def __init__(self, model_path=None):\n        self.model = None\n        self.vectorizer = None\n        self.model_path = model_path or 'models/detector_model.pkl'\n        self.toxic_keywords = [\n            'hate', 'stupid', 'idiot', 'kill', 'die', 'threat',\n            'abuse', 'insult', 'discriminate', 'racist', 'sexist'\n        ]\n        self.load_model()\n    \n    def preprocess_text(self, text):\n        """Clean and preprocess text data"""\n        text = text.lower()\n        text = re.sub(r'[^a-zA-Z\s]', '', text)\n        text = re.sub(r'\s+', ' ', text).strip()\n        return text\n    \n    def extract_features(self, text):\n        """Extract features from text"""\n        text = self.preprocess_text(text)\n        if self.vectorizer is None:\n            self.vectorizer = TfidfVectorizer(max_features=1000)\n        return self.vectorizer.fit_transform([text])\n    \n    def detect_cyberbullying(self, text, threshold=0.5):\n        """Detect if text contains cyberbullying content"""\n        if not text or len(text.strip()) == 0:\n            return {'is_bullying': False, 'confidence': 0.0, 'severity': 'none'}\n        \n        # Keyword-based detection\n        text_lower = text.lower()\n        keyword_score = sum(1 for keyword in self.toxic_keywords if keyword in text_lower) / len(self.toxic_keywords)\n        \n        # Pattern-based detection (ALL CAPS, repeated characters, etc.)\n        caps_ratio = sum(1 for c in text if c.isupper()) / max(len(text), 1)\n        pattern_score = 0.3 if caps_ratio > 0.5 else 0\n        \n        # ML-based detection\n        features = self.extract_features(text)\n        ml_score = self.model.predict_proba(features)[0][1] if self.model else 0.0\n        \n        # Combined score\n        combined_score = (keyword_score * 0.3) + (pattern_score * 0.2) + (ml_score * 0.5)\n        \n        return {\n            'is_bullying': combined_score > threshold,\n            'confidence': float(combined_score),\n            'severity': self.get_severity(combined_score),\n            'timestamp': datetime.now().isoformat()\n        }\n    \n    def get_severity(self, score):\n        """Classify severity based on score"""\n        if score < 0.3:\n            return 'low'\n        elif score < 0.6:\n            return 'medium'\n        else:\n            return 'high'\n    \n    def load_model(self):\n        """Load pre-trained model if available"""\n        try:\n            if os.path.exists(self.model_path):\n                with open(self.model_path, 'rb') as f:\n                    self.model = pickle.load(f)\n        except Exception as e:\n            print(f'Error loading model: {e}')\n            self.model = RandomForestClassifier(n_estimators=100, random_state=42)\n    \n    def save_model(self):\n        """Save trained model"""\n        os.makedirs(os.path.dirname(self.model_path), exist_ok=True)\n        with open(self.model_path, 'wb') as f:\n            pickle.dump(self.model, f)\n    \n    def batch_detect(self, texts):\n        """Detect cyberbullying in multiple texts"""\n        results = []\n        for text in texts:\n            results.append(self.detect_cyberbullying(text))\n        return results\n\n\nif __name__ == '__main__':\n    detector = CyberbullyingDetector()\n    test_text = "This is a test message"\n    result = detector.detect_cyberbullying(test_text)\n    print(f'Detection Result: {result}')\n
